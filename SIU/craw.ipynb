{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 트위터 크롤링"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from twitter_api import BEARER_TOKEN\n",
    "from collections import Counter\n",
    "\n",
    "import nltk\n",
    "import tweepy\n",
    "import pandas as pd\n",
    "from bs4 import BeautifulSoup\n",
    "import requests\n",
    "from konlpy.tag import Okt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "client = tweepy.Client(BEARER_TOKEN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 크롤링 하기\n",
    "query = \"#endangered -is:retweet lang:en\"\n",
    "tweets_en = tweepy.Paginator(\n",
    "    client.search_recent_tweets, query=query, tweet_fields=[\"context_annotations\", \"created_at\"], max_results=100\n",
    ").flatten(limit=1000)\n",
    "\n",
    "# 파일에 쓰기\n",
    "f = open(\"tweet_en.txt\", 'w', encoding=\"utf-8\")\n",
    "for tweet in tweets_en:\n",
    "    f.write(tweet.text)\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "query = \"#멸종 -is:retweet lang:ko\"\n",
    "tweets_ko = tweepy.Paginator(\n",
    "    client.search_recent_tweets, query=query, tweet_fields=[\"context_annotations\", \"created_at\"], max_results=100\n",
    ").flatten(limit=1000)\n",
    "\n",
    "# 파일에 쓰기\n",
    "f = open(\"tweet_ko.txt\", 'w', encoding=\"utf-8\")\n",
    "for tweet in tweets_ko:\n",
    "    f.write(tweet.text)\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "client = tweepy.Client(BEARER_TOKEN)\n",
    "\n",
    "query = \"#animalwelfare -is:retweet lang:en\"\n",
    "tweets_ko = tweepy.Paginator(\n",
    "    client.search_recent_tweets, query=query, tweet_fields=[\"context_annotations\", \"created_at\"], max_results=100\n",
    ").flatten(limit=1000)\n",
    "\n",
    "# 파일에 쓰기\n",
    "f = open(\"병준.txt\", 'w', encoding=\"utf-8\")\n",
    "for tweet in tweets_ko:\n",
    "    f.write(tweet.text)\n",
    "f.close()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 형태소 분석"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\User\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\User\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     C:\\Users\\User\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Unzipping taggers\\averaged_perceptron_tagger.zip.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 모듈 다운\n",
    "# nltk.download('punkt')\n",
    "# nltk.download('stopwords')\n",
    "# nltk.download('averaged_perceptron_tagger')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 파일 읽기\n",
    "with open(\"병준.txt\", \"r\", encoding=\"utf-8\") as f:\n",
    "    data = f.read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 토큰화\n",
    "data = data.lower()\n",
    "token = nltk.word_tokenize(data)\n",
    "\n",
    "# 불용어 처리\n",
    "import string\n",
    "stopwords = nltk.corpus.stopwords.words('english') + list(string.punctuation)\n",
    "token = [t for t in token if t.lower() not in stopwords]\n",
    "\n",
    "# 형태소 태깅\n",
    "tag = nltk.tag.pos_tag(token)\n",
    "word_data = []\n",
    "for word, tag in tag:\n",
    "    if tag in [\"NN\", \"NNS\", \"NNPS\", \"VB\", \"VBD\", \"VBG\", \"VBN\", 'VBZ', \"JJ\", 'JJR', 'JJS']:\n",
    "        word_data.append(word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 숫자 세기\n",
    "word_data = Counter(word_data)\n",
    "make_word_cloud = word_data.most_common(70)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "make_csv = pd.DataFrame(make_word_cloud)\n",
    "make_csv.to_csv(\"병준.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 한국 트윗 형태소 분석"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 형태소 테깅\n",
    "pos = open(\"tweet_ko.txt\", encoding='utf-8').read()\n",
    "okt = Okt()\n",
    "\n",
    "sentences_tag = []\n",
    "sentences_tag = okt.pos(pos)\n",
    "\n",
    "noun_adj_list = []\n",
    "for word, tag in sentences_tag:\n",
    "    if tag in [\"Noun\", \"Adjective\"]:\n",
    "        noun_adj_list.append(word)\n",
    "\n",
    "counts = Counter(noun_adj_list)\n",
    "tags = counts.most_common(70)\n",
    "\n",
    "make_csv = pd.DataFrame(tags)\n",
    "make_csv.to_csv(\"tweet_ko_word.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 한국 뉴스 크롤링"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NaverNews:\n",
    "    url = 'https://search.naver.com/search.naver?where=news&sm=tab_pge'\n",
    "\n",
    "    def __init__(self, search, sort, name=''):\n",
    "        search = '&query=' + str(search)\n",
    "        sort = '&sort=' + str(sort)\n",
    "        self.search = search\n",
    "        self.sort = sort\n",
    "        self.name = name\n",
    "        self.search_url = NaverNews.url + search + sort\n",
    "\n",
    "    \n",
    "    def find_tum_text(self, maxpage, tumClass = 'api_txt_lines dsc_txt_wrap'):\n",
    "\n",
    "        result_list = []\n",
    "\n",
    "        for num in range(1, maxpage+1):\n",
    "            page_num = '&start=' + str(10*num-9)\n",
    "            full_url = self.search_url + page_num \n",
    "            req = requests.get(full_url)\n",
    "            bs = BeautifulSoup(req.text, \"html.parser\")\n",
    "\n",
    "            tums = bs.find_all('a', {'class': tumClass})\n",
    "\n",
    "            for tum in tums:\n",
    "                result_list.append(tum.text)\n",
    "        \n",
    "        return result_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_html(url):\n",
    "        try:\n",
    "            req = requests.get(url)\n",
    "        except requests.exceptions.RequestException:\n",
    "            return None\n",
    "\n",
    "        return BeautifulSoup(req.text, \"html.parser\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def daum_web_find(search, maxpage=1):  \n",
    "    url = 'https://search.daum.net/search?nil_suggest=btn&w=fusion&DA=SBC&q=' + search\n",
    "    result = []\n",
    "\n",
    "    for i in range(1,maxpage+1):\n",
    "        url2 = url + '&p=' + str(i)\n",
    "        html = get_html(url2)\n",
    "\n",
    "        a = html.find_all('p',{'class': 'desc'})\n",
    "        for i in a:\n",
    "            result.append(i.text)    \n",
    "    \n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def to_txt(list, filename):\n",
    "    with open(filename,'w',encoding='UTF-8') as f:\n",
    "        for txt in list:\n",
    "            f.write(txt+'\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "result = NaverNews('동물+등록제', 0)\n",
    "result = result.find_tum_text(100)\n",
    "to_txt(result, \"병준2.txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 형태소 테깅\n",
    "pos = open(\"병준2.txt\", encoding='utf-8').read()\n",
    "okt = Okt()\n",
    "\n",
    "sentences_tag = []\n",
    "sentences_tag = okt.pos(pos)\n",
    "\n",
    "noun_adj_list = []\n",
    "for word, tag in sentences_tag:\n",
    "    if tag in [\"Noun\", \"Adjective\"]:\n",
    "        noun_adj_list.append(word)\n",
    "\n",
    "counts = Counter(noun_adj_list)\n",
    "tags = counts.most_common(70)\n",
    "\n",
    "make_csv = pd.DataFrame(tags)\n",
    "make_csv.to_csv(\"병준2.csv\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.12 ('EV_PY39')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "c34f60c4b6508d34c0193a1e5ad16bfc3f81e2e087a9e2f73ef284bcb9806a0b"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
